{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning en Databricks: Experiencia Completa ğŸ¤–\n",
    "\n",
    "## ğŸ“š Objetivos de Aprendizaje\n",
    "\n",
    "En este notebook aprenderÃ¡s:\n",
    "1. El ecosistema completo de ML en Databricks\n",
    "2. Databricks Runtime for Machine Learning\n",
    "3. AutoML: Machine Learning automatizado\n",
    "4. Feature Store: GestiÃ³n de features\n",
    "5. MLflow: Tracking y experimentaciÃ³n\n",
    "6. Model Serving: Deployment en producciÃ³n\n",
    "7. IntegraciÃ³n con frameworks populares\n",
    "8. Workflows de ML end-to-end\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Â¿QuÃ© hace especial a Databricks para ML?\n",
    "\n",
    "### El Problema Tradicional:\n",
    "\n",
    "```\n",
    "Sin Databricks:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  âŒ MÃºltiples herramientas desconectadas               â”‚\n",
    "â”‚  âŒ Datos en silos separados                            â”‚\n",
    "â”‚  âŒ Dificultad para escalar                             â”‚\n",
    "â”‚  âŒ Deployment complejo                                 â”‚\n",
    "â”‚  âŒ Falta de colaboraciÃ³n                               â”‚\n",
    "â”‚  âŒ Sin reproducibilidad                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### La SoluciÃ³n Databricks:\n",
    "\n",
    "```\n",
    "Con Databricks:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  âœ… Plataforma unificada                                â”‚\n",
    "â”‚  âœ… Datos accesibles desde un solo lugar                â”‚\n",
    "â”‚  âœ… Escalabilidad automÃ¡tica                            â”‚\n",
    "â”‚  âœ… Deployment con un click                             â”‚\n",
    "â”‚  âœ… ColaboraciÃ³n en tiempo real                         â”‚\n",
    "â”‚  âœ… Reproducibilidad garantizada (MLflow)               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Ventajas Clave:\n",
    "\n",
    "1. **ğŸš€ Velocidad**: Del prototipo a producciÃ³n en minutos\n",
    "2. **ğŸ“ˆ Escalabilidad**: De GB a PB sin cambiar cÃ³digo\n",
    "3. **ğŸ¤ ColaboraciÃ³n**: Data Scientists, Engineers y Analysts juntos\n",
    "4. **ğŸ”„ MLOps Integrado**: Todo el ciclo de vida en un solo lugar\n",
    "5. **ğŸ¯ Gobernanza**: Unity Catalog para seguridad y compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ï¸ Arquitectura de ML en Databricks\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DATABRICKS ML PLATFORM                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                              â”‚\n",
    "â”‚  1ï¸âƒ£ DATA LAYER                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ Delta Lake   â”‚  â”‚ Unity Catalogâ”‚  â”‚ Feature Storeâ”‚      â”‚\n",
    "â”‚  â”‚ (Storage)    â”‚  â”‚ (Governance) â”‚  â”‚ (Features)   â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  2ï¸âƒ£ COMPUTE LAYER                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ ML Runtime   â”‚  â”‚ Photon       â”‚  â”‚ GPU Clusters â”‚      â”‚\n",
    "â”‚  â”‚ (Pre-loaded) â”‚  â”‚ (Fast)       â”‚  â”‚ (Deep Learn) â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  3ï¸âƒ£ ML TOOLS                                                â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ AutoML       â”‚  â”‚ MLflow       â”‚  â”‚ Model Servingâ”‚      â”‚\n",
    "â”‚  â”‚ (Auto Train) â”‚  â”‚ (Tracking)   â”‚  â”‚ (Deploy)     â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  4ï¸âƒ£ DEVELOPMENT                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ Notebooks    â”‚  â”‚ Repos (Git)  â”‚  â”‚ Workflows    â”‚      â”‚\n",
    "â”‚  â”‚ (Interactive)â”‚  â”‚ (Version)    â”‚  â”‚ (Orchestrate)â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  5ï¸âƒ£ FRAMEWORKS (Todos soportados)                          â”‚\n",
    "â”‚  scikit-learn | TensorFlow | PyTorch | XGBoost | Spark ML  â”‚\n",
    "â”‚  Keras | LightGBM | Prophet | Hugging Face | Ray           â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš™ï¸ Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports principales\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Verificar versiones\n",
    "print(\"ğŸ”§ ENTORNO DE ML EN DATABRICKS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Verificar librerÃ­as ML disponibles\n",
    "print(\"\\nğŸ“¦ LIBRERÃAS ML PRE-INSTALADAS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "libraries = [\n",
    "    ('sklearn', 'scikit-learn'),\n",
    "    ('xgboost', 'XGBoost'),\n",
    "    ('lightgbm', 'LightGBM'),\n",
    "    ('tensorflow', 'TensorFlow'),\n",
    "    ('torch', 'PyTorch'),\n",
    "]\n",
    "\n",
    "for lib_name, display_name in libraries:\n",
    "    try:\n",
    "        lib = __import__(lib_name)\n",
    "        version = getattr(lib, '__version__', 'N/A')\n",
    "        print(f\"âœ… {display_name:20} v{version}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {display_name:20} No instalado\")\n",
    "\n",
    "# Info del cluster\n",
    "print(\"\\nğŸ–¥ï¸  INFORMACIÃ“N DEL CLUSTER:\")\n",
    "print(\"=\"*60)\n",
    "conf = spark.sparkContext.getConf()\n",
    "print(f\"Tipo de cluster: {conf.get('spark.databricks.clusterUsageTags.clusterType', 'All-Purpose')}\")\n",
    "print(f\"Driver memory: {conf.get('spark.driver.memory', 'Default')}\")\n",
    "print(f\"Executor memory: {conf.get('spark.executor.memory', 'Default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Databricks Runtime for Machine Learning\n",
    "\n",
    "### Â¿QuÃ© es ML Runtime?\n",
    "\n",
    "Es una versiÃ³n especializada de Databricks Runtime que incluye:\n",
    "\n",
    "#### LibrerÃ­as Pre-instaladas:\n",
    "```\n",
    "Machine Learning:\n",
    "  âœ… scikit-learn      âœ… XGBoost          âœ… LightGBM\n",
    "  âœ… TensorFlow        âœ… PyTorch          âœ… Keras\n",
    "  âœ… Spark MLlib       âœ… Prophet          âœ… statsmodels\n",
    "\n",
    "Deep Learning:\n",
    "  âœ… Transformers      âœ… torchvision      âœ… tensorflow-hub\n",
    "  âœ… Horovod (distributed training)\n",
    "\n",
    "AutoML:\n",
    "  âœ… Databricks AutoML âœ… Hyperopt         âœ… MLflow\n",
    "\n",
    "VisualizaciÃ³n:\n",
    "  âœ… matplotlib        âœ… seaborn          âœ… plotly\n",
    "  âœ… tensorboard\n",
    "```\n",
    "\n",
    "#### Optimizaciones:\n",
    "- **ğŸš€ GPU Support**: Para deep learning\n",
    "- **âš¡ Photon Engine**: Acelera operaciones de datos\n",
    "- **ğŸ”§ Distributed Training**: Horovod pre-configurado\n",
    "- **ğŸ“¦ Conda/pip**: InstalaciÃ³n de librerÃ­as adicionales\n",
    "\n",
    "### Tipos de ML Runtime:\n",
    "\n",
    "| Runtime | GPU | Ideal para |\n",
    "|---------|-----|------------|\n",
    "| **ML Runtime** | âŒ | ML tradicional (sklearn, XGBoost) |\n",
    "| **ML Runtime GPU** | âœ… | Deep Learning (TensorFlow, PyTorch) |\n",
    "| **ML Runtime Photon** | âŒ | ML + Data Engineering rÃ¡pido |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DemostraciÃ³n: Importar todos los frameworks principales\n",
    "\n",
    "print(\"ğŸ¯ FRAMEWORKS DE ML DISPONIBLES:\\n\")\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "print(\"âœ… scikit-learn: RandomForest, Logistic Regression\")\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "print(\"âœ… XGBoost: Gradient Boosting\")\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "print(\"âœ… LightGBM: Gradient Boosting\")\n",
    "\n",
    "# TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(f\"âœ… TensorFlow/Keras: Deep Learning (v{tf.__version__})\")\n",
    "except:\n",
    "    print(\"âš ï¸  TensorFlow no disponible (requiere GPU runtime)\")\n",
    "\n",
    "# PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ… PyTorch: Deep Learning (v{torch.__version__})\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   ğŸ® GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"   ğŸ’» CPU only (sin GPU)\")\n",
    "except:\n",
    "    print(\"âš ï¸  PyTorch no disponible\")\n",
    "\n",
    "# Spark MLlib\n",
    "from pyspark.ml.classification import RandomForestClassifier as SparkRF\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "print(\"âœ… Spark MLlib: ML distribuido\")\n",
    "\n",
    "print(\"\\nğŸ‰ Todos los frameworks listos para usar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ AutoML: Machine Learning Automatizado\n",
    "\n",
    "### Â¿QuÃ© es Databricks AutoML?\n",
    "\n",
    "AutoML entrena automÃ¡ticamente mÃºltiples modelos y selecciona el mejor.\n",
    "\n",
    "```\n",
    "Tu provees:                AutoML hace:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Dataset      â”‚  â”€â”€â”€â”€â”€â”€â”€>â”‚ 1. Limpieza de datos        â”‚\n",
    "â”‚ Target       â”‚          â”‚ 2. Feature engineering      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ 3. Prueba mÃºltiples modelos â”‚\n",
    "                          â”‚ 4. Hyperparameter tuning    â”‚\n",
    "                          â”‚ 5. Selecciona el mejor      â”‚\n",
    "                          â”‚ 6. Genera notebook          â”‚\n",
    "                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Tipos de Problemas Soportados:\n",
    "\n",
    "- **ClasificaciÃ³n**: Binaria y multiclase\n",
    "- **RegresiÃ³n**: PredicciÃ³n numÃ©rica\n",
    "- **Forecasting**: Series temporales (con Prophet)\n",
    "\n",
    "### Ventajas:\n",
    "\n",
    "âœ… **RÃ¡pido**: Baseline en minutos  \n",
    "âœ… **Aprende**: Genera notebook con cÃ³digo editable  \n",
    "âœ… **Completo**: Incluye data exploration y feature importance  \n",
    "âœ… **MLflow**: Todos los experimentos rastreados  \n",
    "âœ… **ProducciÃ³n**: Modelos listos para deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para AutoML\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generar dataset de ejemplo\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=15,\n",
    "    n_informative=12,\n",
    "    n_redundant=3,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Crear DataFrame\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Convertir a Spark DataFrame y guardar como tabla\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Crear database\n",
    "username = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "database_name = f\"ml_demo_{username.split('@')[0].replace('.', '_')}\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# Guardar tabla\n",
    "table_name = f\"{database_name}.automl_demo_data\"\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Datos preparados para AutoML\")\n",
    "print(f\"ğŸ“Š Dataset: {len(df):,} muestras, {len(feature_names)} features\")\n",
    "print(f\"ğŸ“ Tabla creada: {table_name}\")\n",
    "print(f\"\\nğŸ¯ DistribuciÃ³n del target:\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ CÃ³mo usar AutoML (Interfaz UI):\n",
    "\n",
    "**OpciÃ³n 1: Desde la UI (Recomendado para principiantes)**\n",
    "\n",
    "1. Ve a **Machine Learning** en el menÃº lateral\n",
    "2. Click en **AutoML** \n",
    "3. Selecciona tu tabla de datos\n",
    "4. Elige la columna target\n",
    "5. Selecciona tipo de problema (Classification/Regression)\n",
    "6. Click **Start AutoML**\n",
    "\n",
    "**AutoML generarÃ¡:**\n",
    "- Notebook con cÃ³digo completo\n",
    "- Data exploration notebook\n",
    "- MÃºltiples modelos entrenados\n",
    "- ComparaciÃ³n de modelos en MLflow\n",
    "- Mejor modelo registrado en Model Registry\n",
    "\n",
    "**OpciÃ³n 2: Desde cÃ³digo (API)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML desde Python API\n",
    "from databricks import automl\n",
    "\n",
    "# Ejecutar AutoML\n",
    "print(\"ğŸ¤– Iniciando AutoML...\\n\")\n",
    "print(\"Esto puede tomar varios minutos...\")\n",
    "print(\"AutoML probarÃ¡ mÃºltiples modelos y configuraciones\\n\")\n",
    "\n",
    "# Nota: Descomenta para ejecutar\n",
    "# summary = automl.classify(\n",
    "#     dataset=spark_df,\n",
    "#     target_col=\"target\",\n",
    "#     primary_metric=\"f1\",\n",
    "#     timeout_minutes=15,\n",
    "#     max_trials=10\n",
    "# )\n",
    "\n",
    "# Ver resultados\n",
    "# print(f\"âœ… AutoML completado!\")\n",
    "# print(f\"Mejor modelo: {summary.best_trial.model_description}\")\n",
    "# print(f\"Mejor F1-Score: {summary.best_trial.metrics['val_f1_score']:.4f}\")\n",
    "# print(f\"\\nExperiment ID: {summary.experiment.experiment_id}\")\n",
    "# print(f\"Notebook generado: {summary.best_trial.notebook_url}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ NOTA: Ejecuta AutoML desde la UI para ver todos los resultados interactivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š QuÃ© incluye el notebook generado por AutoML:\n",
    "\n",
    "```python\n",
    "# 1. Data Exploration\n",
    "- EstadÃ­sticas descriptivas\n",
    "- DistribuciÃ³n de features\n",
    "- Correlaciones\n",
    "- Missing values\n",
    "\n",
    "# 2. Feature Engineering\n",
    "- Encoding de categÃ³ricas\n",
    "- Scaling/Normalization\n",
    "- Feature selection\n",
    "\n",
    "# 3. Model Training\n",
    "- MÃºltiples algoritmos probados\n",
    "- Hyperparameter tuning\n",
    "- Cross-validation\n",
    "\n",
    "# 4. Evaluation\n",
    "- MÃ©tricas de performance\n",
    "- Feature importance\n",
    "- Confusion matrix\n",
    "- ROC curve\n",
    "\n",
    "# 5. MLflow Integration\n",
    "- Todos los runs rastreados\n",
    "- Mejor modelo registrado\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Feature Store: GestiÃ³n de Features\n",
    "\n",
    "### Â¿QuÃ© es Feature Store?\n",
    "\n",
    "Un repositorio centralizado para features de ML que resuelve:\n",
    "\n",
    "```\n",
    "Problemas sin Feature Store:\n",
    "âŒ Features duplicadas entre proyectos\n",
    "âŒ Inconsistencia entre training y serving\n",
    "âŒ Recomputar features constantemente\n",
    "âŒ Sin documentaciÃ³n de features\n",
    "âŒ DifÃ­cil compartir entre equipos\n",
    "\n",
    "Con Feature Store:\n",
    "âœ… Features compartidas y reutilizables\n",
    "âœ… Consistencia garantizada\n",
    "âœ… Features pre-computadas\n",
    "âœ… DocumentaciÃ³n automÃ¡tica\n",
    "âœ… ColaboraciÃ³n entre equipos\n",
    "```\n",
    "\n",
    "### Arquitectura del Feature Store:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              FEATURE STORE                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                     â”‚\n",
    "â”‚  Feature Tables:                                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚ User        â”‚  â”‚ Product     â”‚  â”‚ Transactionâ”‚ â”‚\n",
    "â”‚  â”‚ Features    â”‚  â”‚ Features    â”‚  â”‚ Features   â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  Metadata:                                          â”‚\n",
    "â”‚  - Feature schema                                   â”‚\n",
    "â”‚  - Lineage (quÃ© modelos usan quÃ© features)         â”‚\n",
    "â”‚  - Versioning                                       â”‚\n",
    "â”‚  - Documentation                                    â”‚\n",
    "â”‚                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“                    â†“                â†“\n",
    "    Training            Batch Scoring    Real-time Serving\n",
    "```\n",
    "\n",
    "### Beneficios:\n",
    "\n",
    "1. **ğŸ”„ ReutilizaciÃ³n**: Escribe una vez, usa en mÃºltiples modelos\n",
    "2. **âš¡ Performance**: Features pre-computadas\n",
    "3. **ğŸ“Š Consistencia**: Mismas features en training y producciÃ³n\n",
    "4. **ğŸ“š Descubrimiento**: CatÃ¡logo de features disponibles\n",
    "5. **ğŸ”— Lineage**: Rastrea quÃ© modelos usan quÃ© features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajar con Feature Store\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Inicializar cliente\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "print(\"âœ… Feature Engineering Client inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO: Crear una Feature Table\n",
    "\n",
    "# Generar datos de ejemplo: caracterÃ­sticas de clientes\n",
    "customer_data = spark.createDataFrame([\n",
    "    (\"C001\", 25, 1200.0, 5, \"Premium\"),\n",
    "    (\"C002\", 35, 800.0, 12, \"Standard\"),\n",
    "    (\"C003\", 45, 1500.0, 24, \"Premium\"),\n",
    "    (\"C004\", 28, 600.0, 3, \"Basic\"),\n",
    "    (\"C005\", 52, 2000.0, 36, \"Premium\"),\n",
    "], [\"customer_id\", \"age\", \"monthly_spend\", \"tenure_months\", \"tier\"])\n",
    "\n",
    "# Feature Engineering: Calcular features derivadas\n",
    "customer_features = customer_data.select(\n",
    "    \"customer_id\",\n",
    "    F.current_timestamp().alias(\"feature_timestamp\"),\n",
    "    \n",
    "    # Features base\n",
    "    F.col(\"age\"),\n",
    "    F.col(\"monthly_spend\"),\n",
    "    F.col(\"tenure_months\"),\n",
    "    \n",
    "    # Features derivadas\n",
    "    (F.col(\"monthly_spend\") / F.col(\"tenure_months\")).alias(\"spend_per_month\"),\n",
    "    F.when(F.col(\"tenure_months\") < 6, \"new\")\n",
    "     .when(F.col(\"tenure_months\") < 24, \"regular\")\n",
    "     .otherwise(\"loyal\").alias(\"customer_segment\"),\n",
    "    F.when(F.col(\"tier\") == \"Premium\", 1).otherwise(0).alias(\"is_premium\"),\n",
    "    \n",
    "    # Score de valor del cliente (ejemplo)\n",
    "    ((F.col(\"monthly_spend\") * F.col(\"tenure_months\")) / 1000).alias(\"customer_value_score\")\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Features calculadas:\")\n",
    "display(customer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Feature Table en el Feature Store\n",
    "feature_table_name = f\"{database_name}.customer_features\"\n",
    "\n",
    "try:\n",
    "    # Crear la feature table\n",
    "    fe.create_table(\n",
    "        name=feature_table_name,\n",
    "        primary_keys=[\"customer_id\"],\n",
    "        timestamp_keys=[\"feature_timestamp\"],\n",
    "        df=customer_features,\n",
    "        description=\"Features de clientes para modelos de ML\"\n",
    "    )\n",
    "    print(f\"âœ… Feature table creada: {feature_table_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"âš ï¸  Feature table ya existe, actualizando...\")\n",
    "        fe.write_table(\n",
    "            name=feature_table_name,\n",
    "            df=customer_features,\n",
    "            mode=\"merge\"\n",
    "        )\n",
    "        print(f\"âœ… Feature table actualizada: {feature_table_name}\")\n",
    "    else:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# Leer features del Feature Store\n",
    "features_df = fe.read_table(name=feature_table_name)\n",
    "\n",
    "print(f\"\\nğŸ“– Features disponibles en el Feature Store:\")\n",
    "print(f\"   Columnas: {', '.join(features_df.columns)}\")\n",
    "print(f\"   Total features: {len(features_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Feature Store en la UI:\n",
    "\n",
    "Navega a **Machine Learning > Feature Store** para ver:\n",
    "\n",
    "- ğŸ“Š Lista de todas las feature tables\n",
    "- ğŸ“ Schemas y descripciones\n",
    "- ğŸ”— Lineage: quÃ© modelos usan cada feature\n",
    "- ğŸ“ˆ EstadÃ­sticas y perfiles de datos\n",
    "- â° Historial de actualizaciones\n",
    "\n",
    "### Casos de Uso Comunes:\n",
    "\n",
    "```python\n",
    "# 1. Features de usuario\n",
    "user_features = [\n",
    "    'age', 'gender', 'location',\n",
    "    'signup_date', 'total_purchases',\n",
    "    'avg_purchase_value', 'days_since_last_purchase'\n",
    "]\n",
    "\n",
    "# 2. Features de producto\n",
    "product_features = [\n",
    "    'category', 'price', 'rating',\n",
    "    'num_reviews', 'in_stock',\n",
    "    'days_since_launch', 'popularity_score'\n",
    "]\n",
    "\n",
    "# 3. Features de tiempo\n",
    "time_features = [\n",
    "    'hour_of_day', 'day_of_week',\n",
    "    'is_weekend', 'is_holiday',\n",
    "    'season', 'month'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ MLflow: ExperimentaciÃ³n y Tracking\n",
    "\n",
    "Ya vimos MLflow en detalle en el notebook anterior, aquÃ­ un recordatorio rÃ¡pido:\n",
    "\n",
    "### Componentes de MLflow en Databricks:\n",
    "\n",
    "```\n",
    "1ï¸âƒ£ Tracking\n",
    "   - Registra experimentos automÃ¡ticamente\n",
    "   - Compara mÃºltiples runs\n",
    "   - UI integrada\n",
    "\n",
    "2ï¸âƒ£ Models\n",
    "   - Empaqueta modelos en formato estÃ¡ndar\n",
    "   - Soporta todos los frameworks\n",
    "   - Incluye dependencias\n",
    "\n",
    "3ï¸âƒ£ Model Registry\n",
    "   - Versionado automÃ¡tico\n",
    "   - Stages: Staging â†’ Production\n",
    "   - ColaboraciÃ³n en modelos\n",
    "   - Aprobaciones y comentarios\n",
    "\n",
    "4ï¸âƒ£ Projects\n",
    "   - CÃ³digo reproducible\n",
    "   - Dependencias explÃ­citas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo rÃ¡pido de MLflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Configurar experimento\n",
    "experiment_name = f\"/Users/{username}/ml_experience_demo\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Preparar datos\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar con MLflow\n",
    "with mlflow.start_run(run_name=\"rf_demo\") as run:\n",
    "    \n",
    "    # ParÃ¡metros\n",
    "    params = {\"n_estimators\": 100, \"max_depth\": 10}\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Entrenar\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Log mÃ©tricas\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy, \"f1_score\": f1})\n",
    "    \n",
    "    # Log modelo\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(f\"âœ… Modelo entrenado y rastreado en MLflow\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    print(f\"\\nğŸ”— Ver en MLflow: Machine Learning > Experiments > {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Model Serving: Deployment en ProducciÃ³n\n",
    "\n",
    "### Â¿QuÃ© es Model Serving?\n",
    "\n",
    "Un servicio managed que despliega tu modelo como REST API en segundos.\n",
    "\n",
    "```\n",
    "Sin Model Serving:                Con Model Serving:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Setup infrastructureâ”‚          â”‚ 1 click en la UI     â”‚\n",
    "â”‚ Configure servers   â”‚          â”‚ âœ… Listo            â”‚\n",
    "â”‚ Load balancing      â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚ Monitoring          â”‚\n",
    "â”‚ Scaling             â”‚          Auto-scaling âœ…\n",
    "â”‚ Security            â”‚          Monitoring âœ…\n",
    "â”‚ ...varios dÃ­as...   â”‚          Auth âœ…\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          Versioning âœ…\n",
    "```\n",
    "\n",
    "### CaracterÃ­sticas:\n",
    "\n",
    "âœ… **Serverless**: No gestionar infraestructura  \n",
    "âœ… **Auto-scaling**: Escala segÃºn demanda  \n",
    "âœ… **Low-latency**: <100ms typical  \n",
    "âœ… **Multi-model**: MÃºltiples modelos en un endpoint  \n",
    "âœ… **A/B Testing**: Traffic splitting  \n",
    "âœ… **Monitoring**: MÃ©tricas integradas  \n",
    "\n",
    "### Tipos de Endpoints:\n",
    "\n",
    "| Tipo | Latencia | Throughput | Ideal para |\n",
    "|------|----------|------------|------------|\n",
    "| **Serverless** | ~100ms | Medio | TrÃ¡fico variable |\n",
    "| **Classic** | ~50ms | Alto | TrÃ¡fico constante |\n",
    "| **GPU** | ~20ms | Alto | Deep Learning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ CÃ³mo crear un Model Serving Endpoint:\n",
    "\n",
    "**Desde la UI (MÃ¡s fÃ¡cil):**\n",
    "\n",
    "1. Ve a **Machine Learning > Model Serving**\n",
    "2. Click **Create Serving Endpoint**\n",
    "3. Selecciona el modelo del Model Registry\n",
    "4. Elige el stage (Production/Staging)\n",
    "5. Configura:\n",
    "   - Nombre del endpoint\n",
    "   - Compute size (Small/Medium/Large)\n",
    "   - Scaling config\n",
    "6. Click **Create**\n",
    "\n",
    "Â¡Listo! En ~5 minutos tendrÃ¡s un endpoint REST funcionando.\n",
    "\n",
    "**Features avanzadas:**\n",
    "- ğŸ”„ **Traffic splitting**: A/B testing entre versiones\n",
    "- ğŸ“Š **Monitoring**: Request rate, latency, errors\n",
    "- ğŸ” **Authentication**: Token-based auth\n",
    "- ğŸ“ˆ **Auto-scaling**: Min/max replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cÃ³mo llamar a un Model Serving Endpoint\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# ConfiguraciÃ³n del endpoint (ajusta segÃºn tu endpoint)\n",
    "ENDPOINT_URL = \"https://<workspace-url>/serving-endpoints/<endpoint-name>/invocations\"\n",
    "TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "def predict_from_endpoint(data):\n",
    "    \"\"\"\n",
    "    Hace predicciÃ³n usando Model Serving endpoint\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Formato de los datos\n",
    "    payload = {\n",
    "        \"dataframe_records\": data\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        ENDPOINT_URL,\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Ejemplo de uso (descomenta cuando tengas un endpoint):\n",
    "# test_data = [{\"feature_0\": 1.0, \"feature_1\": 2.0, ...}]\n",
    "# predictions = predict_from_endpoint(test_data)\n",
    "# print(predictions)\n",
    "\n",
    "print(\"ğŸ’¡ NOTA: Crea un Model Serving endpoint desde la UI para usar este cÃ³digo\")\n",
    "print(\"\\nPasos:\")\n",
    "print(\"1. Registra tu modelo en Model Registry\")\n",
    "print(\"2. Ve a Machine Learning > Model Serving\")\n",
    "print(\"3. Crea un endpoint\")\n",
    "print(\"4. Copia la URL del endpoint y actualiza ENDPOINT_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£ IntegraciÃ³n con Frameworks Populares\n",
    "\n",
    "Databricks soporta todos los frameworks principales de ML y Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¬ Scikit-learn (ML Tradicional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"ğŸ”¬ SCIKIT-LEARN\\n\")\n",
    "\n",
    "# Modelo\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "print(f\"âœ… Gradient Boosting entrenado\")\n",
    "print(f\"   CV Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "print(f\"\\nğŸ’¡ Databricks incluye todas las herramientas de sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ XGBoost / LightGBM (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"âš¡ GRADIENT BOOSTING FRAMEWORKS\\n\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_acc = accuracy_score(y_test, xgb_model.predict(X_test))\n",
    "\n",
    "print(f\"âœ… XGBoost accuracy: {xgb_acc:.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_acc = accuracy_score(y_test, lgb_model.predict(X_test))\n",
    "\n",
    "print(f\"âœ… LightGBM accuracy: {lgb_acc:.4f}\")\n",
    "print(f\"\\nğŸ’¡ Ambos optimizados para Databricks (GPU optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  TensorFlow / Keras (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    print(\"ğŸ§  TENSORFLOW / KERAS\\n\")\n",
    "    \n",
    "    # Modelo simple\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Red neuronal creada\")\n",
    "    print(f\"   Capas: {len(model.layers)}\")\n",
    "    print(f\"   ParÃ¡metros: {model.count_params():,}\")\n",
    "    \n",
    "    # Entrenar (pocas epochs para demo)\n",
    "    # history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    # print(f\"\\n   Final accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ TensorFlow + GPU para deep learning en Databricks\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸  TensorFlow requiere ML Runtime GPU\")\n",
    "    print(\"   Usa un cluster con GPU para deep learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¥ PyTorch (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    print(\"ğŸ”¥ PYTORCH\\n\")\n",
    "    \n",
    "    # Modelo simple\n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, 64)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.fc3 = nn.Linear(32, 1)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "    \n",
    "    model = SimpleNN(X_train.shape[1])\n",
    "    \n",
    "    print(\"âœ… Modelo PyTorch creado\")\n",
    "    print(f\"   Arquitectura: {sum(p.numel() for p in model.parameters()):,} parÃ¡metros\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   ğŸ® GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"   ğŸ’» CPU mode\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ PyTorch + Horovod para distributed training\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸  PyTorch no disponible en este runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ Spark MLlib (Distributed ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier as SparkRF\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "print(\"âš¡ SPARK MLLIB (Distributed ML)\\n\")\n",
    "\n",
    "# Preparar datos en formato Spark\n",
    "spark_train_df = spark.createDataFrame(\n",
    "    pd.DataFrame(X_train, columns=feature_names).assign(label=y_train)\n",
    ")\n",
    "\n",
    "spark_test_df = spark.createDataFrame(\n",
    "    pd.DataFrame(X_test, columns=feature_names).assign(label=y_test)\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\n",
    "rf = SparkRF(labelCol=\"label\", featuresCol=\"features\", numTrees=100, maxDepth=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Entrenar\n",
    "spark_model = pipeline.fit(spark_train_df)\n",
    "\n",
    "# Evaluar\n",
    "predictions = spark_model.transform(spark_test_df)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"âœ… Spark MLlib Random Forest\")\n",
    "print(f\"   AUC: {auc:.4f}\")\n",
    "print(f\"\\nğŸ’¡ Ideal para datasets masivos (TB/PB)\")\n",
    "print(f\"   Escala automÃ¡ticamente con tu cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Workflows de ML End-to-End\n",
    "\n",
    "### Flujo Completo en Databricks:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ML WORKFLOW                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  1ï¸âƒ£ DATA INGESTION                                         â”‚\n",
    "â”‚     â†“ Data Lake, Databases, APIs, Streams                  â”‚\n",
    "â”‚     â†“ Delta Lake (versioned, ACID)                         â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  2ï¸âƒ£ DATA PREPARATION                                       â”‚\n",
    "â”‚     â†“ Spark for ETL at scale                               â”‚\n",
    "â”‚     â†“ Data cleaning, transformation                        â”‚\n",
    "â”‚     â†“ Feature engineering                                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  3ï¸âƒ£ FEATURE STORE                                          â”‚\n",
    "â”‚     â†“ Centralized features                                 â”‚\n",
    "â”‚     â†“ Reusable across projects                             â”‚\n",
    "â”‚     â†“ Online/Offline serving                               â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  4ï¸âƒ£ EXPERIMENTATION                                        â”‚\n",
    "â”‚     â†“ AutoML for baseline                                  â”‚\n",
    "â”‚     â†“ MLflow tracking                                      â”‚\n",
    "â”‚     â†“ Hyperparameter tuning                                â”‚\n",
    "â”‚     â†“ Compare experiments                                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  5ï¸âƒ£ MODEL REGISTRY                                         â”‚\n",
    "â”‚     â†“ Version models                                       â”‚\n",
    "â”‚     â†“ Staging â†’ Production                                 â”‚\n",
    "â”‚     â†“ Collaboration & approvals                            â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  6ï¸âƒ£ DEPLOYMENT                                             â”‚\n",
    "â”‚     â†“ Model Serving (real-time)                            â”‚\n",
    "â”‚     â†“ Batch inference                                      â”‚\n",
    "â”‚     â†“ Streaming inference                                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  7ï¸âƒ£ MONITORING                                             â”‚\n",
    "â”‚     â†“ Model performance                                    â”‚\n",
    "â”‚     â†“ Data drift detection                                 â”‚\n",
    "â”‚     â†“ Alerts & dashboards                                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  8ï¸âƒ£ AUTOMATION                                             â”‚\n",
    "â”‚     â†“ Workflows/Jobs                                       â”‚\n",
    "â”‚     â†“ Scheduled retraining                                 â”‚\n",
    "â”‚     â†“ CI/CD integration                                    â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ Ejemplo de Workflow Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PseudocÃ³digo de un workflow completo\n",
    "\n",
    "def ml_pipeline():\n",
    "    \"\"\"\n",
    "    Pipeline completo de ML en Databricks\n",
    "    Este cÃ³digo se ejecutarÃ­a como un Databricks Workflow\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. INGESTA DE DATOS\n",
    "    print(\"1ï¸âƒ£ Ingiriendo datos...\")\n",
    "    raw_data = spark.read.format(\"delta\").table(\"raw_data.transactions\")\n",
    "    \n",
    "    # 2. PREPARACIÃ“N\n",
    "    print(\"2ï¸âƒ£ Preparando datos...\")\n",
    "    clean_data = (\n",
    "        raw_data\n",
    "        .filter(F.col(\"amount\") > 0)\n",
    "        .dropDuplicates([\"transaction_id\"])\n",
    "        .fillna({\"category\": \"unknown\"})\n",
    "    )\n",
    "    \n",
    "    # 3. FEATURE ENGINEERING\n",
    "    print(\"3ï¸âƒ£ Creando features...\")\n",
    "    features = (\n",
    "        clean_data\n",
    "        .withColumn(\"hour_of_day\", F.hour(\"timestamp\"))\n",
    "        .withColumn(\"is_weekend\", F.dayofweek(\"timestamp\").isin([6, 7]))\n",
    "        # ... mÃ¡s features\n",
    "    )\n",
    "    \n",
    "    # 4. GUARDAR EN FEATURE STORE\n",
    "    print(\"4ï¸âƒ£ Actualizando Feature Store...\")\n",
    "    fe.write_table(\n",
    "        name=\"features.transaction_features\",\n",
    "        df=features,\n",
    "        mode=\"merge\"\n",
    "    )\n",
    "    \n",
    "    # 5. ENTRENAMIENTO\n",
    "    print(\"5ï¸âƒ£ Entrenando modelo...\")\n",
    "    with mlflow.start_run():\n",
    "        # AutoML o custom training\n",
    "        summary = automl.classify(\n",
    "            dataset=features,\n",
    "            target_col=\"fraud\",\n",
    "            timeout_minutes=30\n",
    "        )\n",
    "    \n",
    "    # 6. VALIDACIÃ“N\n",
    "    print(\"6ï¸âƒ£ Validando modelo...\")\n",
    "    best_model = mlflow.sklearn.load_model(f\"runs:/{summary.best_trial.run_id}/model\")\n",
    "    \n",
    "    # Validar mÃ©tricas\n",
    "    if summary.best_trial.metrics['val_f1_score'] > 0.85:\n",
    "        # 7. PROMOVER A STAGING\n",
    "        print(\"7ï¸âƒ£ Promoviendo a Staging...\")\n",
    "        client = MlflowClient()\n",
    "        model_version = client.get_latest_versions(\"fraud_detector\", stages=[\"None\"])[0]\n",
    "        client.transition_model_version_stage(\n",
    "            name=\"fraud_detector\",\n",
    "            version=model_version.version,\n",
    "            stage=\"Staging\"\n",
    "        )\n",
    "    \n",
    "    print(\"âœ… Pipeline completado!\")\n",
    "\n",
    "# Este workflow se programarÃ­a como un Job en Databricks\n",
    "# EjecutÃ¡ndose diariamente/semanalmente/mensualmente\n",
    "\n",
    "print(\"ğŸ’¡ Este cÃ³digo representa un workflow tÃ­pico de ML\")\n",
    "print(\"   En producciÃ³n, se ejecutarÃ­a como un Databricks Job\")\n",
    "print(\"   con schedule y monitoreo automÃ¡tico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Best Practices de ML en Databricks\n",
    "\n",
    "### 1. OrganizaciÃ³n de Proyectos:\n",
    "\n",
    "```\n",
    "workspace/\n",
    "â”œâ”€â”€ projects/\n",
    "â”‚   â”œâ”€â”€ fraud-detection/\n",
    "â”‚   â”‚   â”œâ”€â”€ 01_data_exploration.py\n",
    "â”‚   â”‚   â”œâ”€â”€ 02_feature_engineering.py\n",
    "â”‚   â”‚   â”œâ”€â”€ 03_model_training.py\n",
    "â”‚   â”‚   â”œâ”€â”€ 04_evaluation.py\n",
    "â”‚   â”‚   â””â”€â”€ 05_deployment.py\n",
    "â”‚   â””â”€â”€ customer-churn/\n",
    "â”‚       â””â”€â”€ ...\n",
    "â”œâ”€â”€ shared/\n",
    "â”‚   â”œâ”€â”€ utils.py\n",
    "â”‚   â””â”€â”€ config.py\n",
    "â””â”€â”€ production/\n",
    "    â”œâ”€â”€ workflows/\n",
    "    â””â”€â”€ monitoring/\n",
    "```\n",
    "\n",
    "### 2. Feature Store:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Documenta cada feature\n",
    "- Usa naming conventions consistentes\n",
    "- Versiona features importantes\n",
    "- Comparte features entre equipos\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Duplicar features\n",
    "- Features sin documentaciÃ³n\n",
    "- Features computacionalmente caras sin cachÃ©\n",
    "\n",
    "### 3. ExperimentaciÃ³n:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Usa AutoML para baseline rÃ¡pido\n",
    "- Rastrea TODOS los experimentos en MLflow\n",
    "- Documenta hiperparÃ¡metros y decisiones\n",
    "- Compara mÃºltiples modelos\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Entrenar sin MLflow tracking\n",
    "- Ignorar el baseline de AutoML\n",
    "- Experimentar sin versiÃ³n de datos\n",
    "\n",
    "### 4. Model Registry:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Siempre usa stages (None â†’ Staging â†’ Production)\n",
    "- Agrega descripciones a cada versiÃ³n\n",
    "- Documenta cambios entre versiones\n",
    "- Usa tags para metadata\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Deployment directo a Production sin Staging\n",
    "- Versiones sin descripciÃ³n\n",
    "- Eliminar modelos en uso\n",
    "\n",
    "### 5. Deployment:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Usa Model Serving para real-time\n",
    "- Monitora latency y throughput\n",
    "- Implementa A/B testing\n",
    "- Configura alertas\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Deploy sin testing en Staging\n",
    "- Ignorar mÃ©tricas de producciÃ³n\n",
    "- Sin plan de rollback\n",
    "\n",
    "### 6. ColaboraciÃ³n:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Usa Git/Repos para cÃ³digo\n",
    "- Comenta notebooks claramente\n",
    "- Comparte experimentos vÃ­a MLflow\n",
    "- Documenta decisiones\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- CÃ³digo sin versionado\n",
    "- Notebooks sin documentaciÃ³n\n",
    "- Trabajo en silos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Casos de Uso Reales\n",
    "\n",
    "### 1. DetecciÃ³n de Fraude en Tiempo Real\n",
    "```\n",
    "Pipeline:\n",
    "1. Streaming data â†’ Delta Lake\n",
    "2. Features en Feature Store (pre-computadas)\n",
    "3. Modelo en Model Serving\n",
    "4. PredicciÃ³n <50ms\n",
    "5. Alertas automÃ¡ticas\n",
    "\n",
    "Stack:\n",
    "- Spark Streaming\n",
    "- Feature Store\n",
    "- XGBoost/LightGBM\n",
    "- Model Serving\n",
    "```\n",
    "\n",
    "### 2. Recomendaciones Personalizadas\n",
    "```\n",
    "Pipeline:\n",
    "1. HistÃ³rico de usuarios â†’ Feature Store\n",
    "2. AutoML para prototipo rÃ¡pido\n",
    "3. Fine-tuning con collaborative filtering\n",
    "4. A/B testing en producciÃ³n\n",
    "5. Reentrenamiento semanal\n",
    "\n",
    "Stack:\n",
    "- Feature Store\n",
    "- AutoML + Spark MLlib\n",
    "- Model Registry\n",
    "- Workflows\n",
    "```\n",
    "\n",
    "### 3. Computer Vision en Manufactura\n",
    "```\n",
    "Pipeline:\n",
    "1. ImÃ¡genes â†’ Delta Lake\n",
    "2. Transfer learning (ResNet/EfficientNet)\n",
    "3. Distributed training con Horovod\n",
    "4. Edge deployment\n",
    "5. Feedback loop continuo\n",
    "\n",
    "Stack:\n",
    "- TensorFlow/PyTorch\n",
    "- GPU clusters\n",
    "- MLflow\n",
    "- Model Registry\n",
    "```\n",
    "\n",
    "### 4. Forecasting de Demanda\n",
    "```\n",
    "Pipeline:\n",
    "1. Time series data â†’ Feature Store\n",
    "2. AutoML con Prophet\n",
    "3. Ensemble de modelos\n",
    "4. Batch predictions diarias\n",
    "5. Dashboard en Databricks SQL\n",
    "\n",
    "Stack:\n",
    "- Prophet/ARIMA\n",
    "- AutoML\n",
    "- Workflows\n",
    "- SQL Dashboards\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Recursos y PrÃ³ximos Pasos\n",
    "\n",
    "### DocumentaciÃ³n Oficial:\n",
    "- [Databricks ML Guide](https://docs.databricks.com/machine-learning/index.html)\n",
    "- [AutoML Documentation](https://docs.databricks.com/applications/machine-learning/automl.html)\n",
    "- [Feature Store Guide](https://docs.databricks.com/machine-learning/feature-store/index.html)\n",
    "- [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html)\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)\n",
    "\n",
    "### Tutoriales y Ejemplos:\n",
    "- Databricks Academy (cursos gratuitos)\n",
    "- [Example Notebooks](https://docs.databricks.com/notebooks/notebook-gallery.html)\n",
    "- Databricks Community Edition (prueba gratuita)\n",
    "\n",
    "### Certificaciones:\n",
    "- Databricks Certified Associate Developer for Apache Spark\n",
    "- Databricks Certified Machine Learning Associate\n",
    "- Databricks Certified Machine Learning Professional\n",
    "\n",
    "### Comunidad:\n",
    "- [Databricks Community Forum](https://community.databricks.com/)\n",
    "- Stack Overflow (tag: databricks)\n",
    "- Databricks Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Resumen Ejecutivo\n",
    "\n",
    "### La Experiencia de ML en Databricks:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  DATABRICKS = PLATAFORMA UNIFICADA DE ML                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                          â”‚\n",
    "â”‚  âœ… Runtime optimizado con todas las librerÃ­as ML       â”‚\n",
    "â”‚  âœ… AutoML para prototipos en minutos                   â”‚\n",
    "â”‚  âœ… Feature Store para features reutilizables           â”‚\n",
    "â”‚  âœ… MLflow integrado para tracking                      â”‚\n",
    "â”‚  âœ… Model Serving para deployment fÃ¡cil                 â”‚\n",
    "â”‚  âœ… Soporta todos los frameworks populares              â”‚\n",
    "â”‚  âœ… Workflows para automatizaciÃ³n                       â”‚\n",
    "â”‚  âœ… Escalabilidad de GB a PB                            â”‚\n",
    "â”‚  âœ… ColaboraciÃ³n en tiempo real                         â”‚\n",
    "â”‚  âœ… Gobernanza con Unity Catalog                        â”‚\n",
    "â”‚                                                          â”‚\n",
    "â”‚  RESULTADO: Del prototipo a producciÃ³n en dÃ­as,         â”‚\n",
    "â”‚             no semanas o meses                           â”‚\n",
    "â”‚                                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Ventajas Competitivas:\n",
    "\n",
    "1. **ğŸš€ Velocidad**: AutoML + pre-built workflows\n",
    "2. **ğŸ“ˆ Escala**: De laptop a cluster sin cambiar cÃ³digo\n",
    "3. **ğŸ¤ ColaboraciÃ³n**: Data Scientists + Engineers + Analysts\n",
    "4. **ğŸ”„ MLOps**: Ciclo completo integrado\n",
    "5. **ğŸ’° ROI**: ReducciÃ³n de time-to-market 10x\n",
    "\n",
    "---\n",
    "\n",
    "**Â¡Has completado el tour de Machine Learning en Databricks!** ğŸ‰\n",
    "\n",
    "Ahora tienes una visiÃ³n completa del ecosistema ML y estÃ¡s listo para:\n",
    "- Entrenar tu primer modelo con AutoML\n",
    "- Crear features en Feature Store\n",
    "- Rastrear experimentos con MLflow\n",
    "- Desplegar modelos con Model Serving\n",
    "- Automatizar workflows de ML\n",
    "\n",
    "Â¡Manos a la obra! ğŸ’ª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
