{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducci√≥n a Databricks üöÄ\n",
    "\n",
    "## üìö Objetivos de Aprendizaje\n",
    "\n",
    "En este notebook aprender√°s:\n",
    "1. ¬øQu√© es Databricks y para qu√© se usa?\n",
    "2. Arquitectura y componentes principales\n",
    "3. C√≥mo usar notebooks en Databricks\n",
    "4. Trabajar con Apache Spark\n",
    "5. Gesti√≥n de datos con Delta Lake\n",
    "6. Exploraci√≥n del workspace\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ¬øQu√© es Databricks?\n",
    "\n",
    "**Databricks** es una plataforma unificada de an√°lisis de datos construida sobre **Apache Spark**. Fue fundada por los creadores originales de Apache Spark.\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "- **Ingenier√≠a de Datos**: Procesar grandes vol√∫menes de datos\n",
    "- **Ciencia de Datos**: Entrenar modelos de Machine Learning\n",
    "- **Analytics**: An√°lisis y visualizaci√≥n de datos\n",
    "- **MLOps**: Desplegar y gestionar modelos en producci√≥n\n",
    "\n",
    "### ¬øPor qu√© Databricks?\n",
    "\n",
    "‚úÖ **Escalable**: Procesa desde GB hasta PB de datos  \n",
    "‚úÖ **Colaborativo**: M√∫ltiples usuarios trabajando juntos  \n",
    "‚úÖ **Integrado**: Conecta con m√∫ltiples fuentes de datos  \n",
    "‚úÖ **Optimizado**: Motor Spark optimizado (Photon)  \n",
    "‚úÖ **Completo**: Todo el ciclo de datos en un solo lugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Arquitectura de Databricks\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    DATABRICKS WORKSPACE                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ  Notebooks   ‚îÇ  ‚îÇ   Workflows  ‚îÇ  ‚îÇ    Jobs      ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ  (C√≥digo)    ‚îÇ  ‚îÇ  (Pipelines) ‚îÇ  ‚îÇ(Automatizaci√≥n)‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ Data Explorer‚îÇ  ‚îÇ  ML Runtime  ‚îÇ  ‚îÇ  Dashboards  ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ  (Cat√°logo)  ‚îÇ  ‚îÇ   (MLflow)   ‚îÇ  ‚îÇ (BI/Reportes)‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                   COMPUTE CLUSTERS                      ‚îÇ\n",
    "‚îÇ            (M√°quinas que procesan los datos)            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                    DELTA LAKE STORAGE                   ‚îÇ\n",
    "‚îÇ              (Almacenamiento de datos)                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Componentes Principales:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Workspace**\n",
    "- Entorno colaborativo donde trabajas\n",
    "- Organiza notebooks, datos y c√≥digo\n",
    "- Similar a Google Drive pero para datos y c√≥digo\n",
    "\n",
    "#### 2Ô∏è‚É£ **Clusters (Compute)**\n",
    "- Conjunto de m√°quinas que procesan tus datos\n",
    "- Puedes escalarlos seg√∫n necesites\n",
    "- Se apagan autom√°ticamente para ahorrar costos\n",
    "\n",
    "#### 3Ô∏è‚É£ **Delta Lake**\n",
    "- Sistema de almacenamiento optimizado\n",
    "- Transacciones ACID (como bases de datos)\n",
    "- Versionado de datos (como Git para datos)\n",
    "\n",
    "#### 4Ô∏è‚É£ **Unity Catalog**\n",
    "- Cat√°logo centralizado de datos\n",
    "- Gobernanza y seguridad\n",
    "- Control de acceso granular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìì Notebooks en Databricks\n",
    "\n",
    "Los notebooks son documentos interactivos donde escribes y ejecutas c√≥digo.\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "\n",
    "- **Multi-lenguaje**: Python, SQL, Scala, R\n",
    "- **Interactivo**: Ejecutas c√≥digo celda por celda\n",
    "- **Colaborativo**: Varios usuarios editando simult√°neamente\n",
    "- **Versionado**: Integraci√≥n con Git\n",
    "\n",
    "### Magic Commands (Comandos M√°gicos):\n",
    "\n",
    "| Comando | Descripci√≥n | Ejemplo |\n",
    "|---------|-------------|----------|\n",
    "| `%python` | Ejecutar c√≥digo Python | `%python print(\"Hello\")` |\n",
    "| `%sql` | Ejecutar consultas SQL | `%sql SELECT * FROM tabla` |\n",
    "| `%scala` | Ejecutar c√≥digo Scala | `%scala println(\"Hello\")` |\n",
    "| `%r` | Ejecutar c√≥digo R | `%r print(\"Hello\")` |\n",
    "| `%sh` | Ejecutar comandos shell | `%sh ls -la` |\n",
    "| `%fs` | Operaciones de filesystem | `%fs ls /data` |\n",
    "| `%md` | Escribir Markdown | `%md # T√≠tulo` |\n",
    "\n",
    "### Utilities (dbutils):\n",
    "\n",
    "Databricks proporciona utilidades especiales:\n",
    "\n",
    "- `dbutils.fs` - Operaciones de archivos\n",
    "- `dbutils.notebook` - Ejecutar otros notebooks\n",
    "- `dbutils.widgets` - Crear widgets interactivos\n",
    "- `dbutils.secrets` - Acceder a secretos seguros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî• ¬°Empecemos! Tu Primer C√≥digo en Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Python b√°sico en Databricks\n",
    "print(\"¬°Hola Databricks! üéâ\")\n",
    "print(f\"Estoy ejecutando c√≥digo en Python {import sys; sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Informaci√≥n del usuario actual\n",
    "current_user = spark.sql(\"SELECT current_user() as user\").collect()[0][0]\n",
    "print(f\"üë§ Usuario actual: {current_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Informaci√≥n del cluster\n",
    "print(\"üñ•Ô∏è Informaci√≥n del Cluster:\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Python Version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Obtener configuraci√≥n del cluster\n",
    "conf = spark.sparkContext.getConf()\n",
    "print(f\"\\n‚öôÔ∏è Configuraci√≥n:\")\n",
    "print(f\"   Driver Memory: {conf.get('spark.driver.memory', 'Default')}\")\n",
    "print(f\"   Executor Memory: {conf.get('spark.executor.memory', 'Default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö° Apache Spark: El Motor de Databricks\n",
    "\n",
    "**Apache Spark** es un motor de procesamiento distribuido de datos.\n",
    "\n",
    "### ¬øQu√© significa \"distribuido\"?\n",
    "\n",
    "En lugar de procesar datos en una sola computadora:\n",
    "\n",
    "```\n",
    "Procesamiento Tradicional (1 m√°quina):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 100 GB   ‚îÇ  ‚Üê Lento, puede crashear\n",
    "‚îÇ de datos ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Procesamiento Distribuido (Spark):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 25GB ‚îÇ  ‚îÇ 25GB ‚îÇ  ‚îÇ 25GB ‚îÇ  ‚îÇ 25GB ‚îÇ  ‚Üê R√°pido, escalable\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "   ‚Üì         ‚Üì         ‚Üì         ‚Üì\n",
    "        Procesamiento paralelo\n",
    "```\n",
    "\n",
    "### Conceptos Clave:\n",
    "\n",
    "#### **DataFrame**\n",
    "- Tabla distribuida de datos (como Pandas pero distribuido)\n",
    "- Optimizado autom√°ticamente por Spark\n",
    "\n",
    "#### **Transformaciones** (Lazy)\n",
    "- Operaciones que definen qu√© hacer con los datos\n",
    "- No se ejecutan inmediatamente\n",
    "- Ejemplos: `select()`, `filter()`, `groupBy()`\n",
    "\n",
    "#### **Acciones** (Eager)\n",
    "- Operaciones que ejecutan las transformaciones\n",
    "- Devuelven resultados\n",
    "- Ejemplos: `show()`, `count()`, `collect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame desde Python\n",
    "import pandas as pd\n",
    "\n",
    "# Datos de ejemplo: Ventas de una tienda\n",
    "data = {\n",
    "    'producto': ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Laptop', 'Mouse', 'Monitor'],\n",
    "    'cantidad': [2, 5, 3, 1, 1, 8, 2],\n",
    "    'precio': [1200, 25, 75, 300, 1200, 25, 300],\n",
    "    'fecha': ['2024-01-15', '2024-01-15', '2024-01-16', '2024-01-16', \n",
    "              '2024-01-17', '2024-01-17', '2024-01-18']\n",
    "}\n",
    "\n",
    "# Convertir a Pandas DataFrame\n",
    "pandas_df = pd.DataFrame(data)\n",
    "\n",
    "# Convertir a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"‚úÖ DataFrame creado\")\n",
    "print(f\"   Filas: {spark_df.count()}\")\n",
    "print(f\"   Columnas: {len(spark_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los datos (action)\n",
    "print(\"üìä Primeras filas del DataFrame:\")\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el esquema (estructura de datos)\n",
    "print(\"üèóÔ∏è Esquema del DataFrame:\")\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMACIONES (no se ejecutan hasta una action)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Agregar columna calculada: total de venta\n",
    "df_con_total = spark_df.withColumn(\n",
    "    \"total\", \n",
    "    F.col(\"cantidad\") * F.col(\"precio\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Transformaci√≥n definida (a√∫n no ejecutada)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION: Ahora s√≠ se ejecuta\n",
    "print(\"üí∞ Ventas con total calculado:\")\n",
    "display(df_con_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√°s transformaciones comunes\n",
    "\n",
    "# 1. FILTRAR datos\n",
    "ventas_altas = df_con_total.filter(F.col(\"total\") > 100)\n",
    "\n",
    "print(\"üîç Ventas mayores a $100:\")\n",
    "display(ventas_altas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. AGRUPAR y AGREGAR\n",
    "ventas_por_producto = df_con_total.groupBy(\"producto\").agg(\n",
    "    F.sum(\"cantidad\").alias(\"total_unidades\"),\n",
    "    F.sum(\"total\").alias(\"ingresos_totales\"),\n",
    "    F.avg(\"precio\").alias(\"precio_promedio\")\n",
    ").orderBy(F.desc(\"ingresos_totales\"))\n",
    "\n",
    "print(\"üìà Resumen por producto:\")\n",
    "display(ventas_por_producto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ORDENAR datos\n",
    "df_ordenado = df_con_total.orderBy(F.desc(\"total\"))\n",
    "\n",
    "print(\"üèÜ Top 3 ventas m√°s altas:\")\n",
    "display(df_ordenado.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Delta Lake: Almacenamiento Inteligente\n",
    "\n",
    "**Delta Lake** es un formato de almacenamiento que a√±ade:\n",
    "\n",
    "- ‚úÖ **Transacciones ACID**: Escrituras seguras y consistentes\n",
    "- ‚úÖ **Time Travel**: Ver versiones anteriores de tus datos\n",
    "- ‚úÖ **Schema Enforcement**: Previene datos incorrectos\n",
    "- ‚úÖ **Merge/Upsert**: Actualizar datos eficientemente\n",
    "- ‚úÖ **Optimizaci√≥n autom√°tica**: Compacta archivos peque√±os\n",
    "\n",
    "### Delta vs Parquet/CSV:\n",
    "\n",
    "```\n",
    "CSV/Parquet:              Delta Lake:\n",
    "‚ùå Sin transacciones      ‚úÖ Transacciones ACID\n",
    "‚ùå Sin versionado         ‚úÖ Time Travel\n",
    "‚ùå Dif√≠cil actualizar     ‚úÖ MERGE/UPDATE/DELETE\n",
    "‚ùå Sin validaci√≥n         ‚úÖ Schema enforcement\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una tabla Delta\n",
    "\n",
    "# Primero, definir d√≥nde guardamos nuestras tablas\n",
    "username = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "database_name = f\"demo_{username.split('@')[0].replace('.', '_')}\"\n",
    "\n",
    "# Crear database si no existe\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "print(f\"üìÅ Usando database: {database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DataFrame como tabla Delta\n",
    "tabla_ventas = \"ventas_demo\"\n",
    "\n",
    "df_con_total.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(tabla_ventas)\n",
    "\n",
    "print(f\"‚úÖ Tabla Delta creada: {database_name}.{tabla_ventas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer desde la tabla Delta\n",
    "df_desde_tabla = spark.table(tabla_ventas)\n",
    "\n",
    "print(\"üìñ Leyendo desde tabla Delta:\")\n",
    "display(df_desde_tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERTAR nuevos datos\n",
    "nuevas_ventas = spark.createDataFrame([\n",
    "    ('Laptop', 3, 1200, '2024-01-19', 3600),\n",
    "    ('Teclado', 2, 75, '2024-01-19', 150)\n",
    "], ['producto', 'cantidad', 'precio', 'fecha', 'total'])\n",
    "\n",
    "nuevas_ventas.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(tabla_ventas)\n",
    "\n",
    "print(\"‚úÖ Datos insertados\")\n",
    "print(f\"   Total de registros ahora: {spark.table(tabla_ventas).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME TRAVEL: Ver versiones anteriores\n",
    "print(\"üìú Historial de versiones:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY {tabla_ventas}\").select(\n",
    "    \"version\", \"timestamp\", \"operation\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver versi√≥n anterior (antes del INSERT)\n",
    "df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(tabla_ventas)\n",
    "\n",
    "print(\"‚è∞ Datos en la versi√≥n 0:\")\n",
    "print(f\"   Registros: {df_version_0.count()}\")\n",
    "display(df_version_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÉÔ∏è SQL en Databricks\n",
    "\n",
    "Databricks soporta SQL est√°ndar con extensiones para Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta SQL b√°sica\n",
    "%sql\n",
    "SELECT \n",
    "    producto,\n",
    "    SUM(cantidad) as total_vendido,\n",
    "    SUM(total) as ingresos\n",
    "FROM ventas_demo\n",
    "GROUP BY producto\n",
    "ORDER BY ingresos DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL con funciones de ventana\n",
    "%sql\n",
    "SELECT \n",
    "    producto,\n",
    "    fecha,\n",
    "    total,\n",
    "    SUM(total) OVER (PARTITION BY producto ORDER BY fecha) as total_acumulado\n",
    "FROM ventas_demo\n",
    "ORDER BY producto, fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambi√©n puedes ejecutar SQL desde Python\n",
    "resultado = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_transacciones,\n",
    "        SUM(total) as ingresos_totales,\n",
    "        AVG(total) as ticket_promedio\n",
    "    FROM ventas_demo\n",
    "\"\"\")\n",
    "\n",
    "print(\"üí∞ M√©tricas de negocio:\")\n",
    "display(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Utilidades de Databricks (dbutils)\n",
    "\n",
    "Databricks proporciona utilidades especiales para operaciones comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Operaciones de archivos\n",
    "print(\"üìÅ Comandos de filesystem:\")\n",
    "print(\"\\nListar archivos en /databricks-datasets (datasets de ejemplo):\")\n",
    "display(dbutils.fs.ls(\"/databricks-datasets\")[:5])  # Primeros 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ver archivos de una tabla Delta\n",
    "print(\"üìÇ Archivos de nuestra tabla Delta:\")\n",
    "\n",
    "# Obtener la ubicaci√≥n de la tabla\n",
    "tabla_info = spark.sql(f\"DESCRIBE DETAIL {tabla_ventas}\").select(\"location\").first()[0]\n",
    "print(f\"\\nUbicaci√≥n: {tabla_info}\")\n",
    "\n",
    "# Listar archivos\n",
    "files = dbutils.fs.ls(tabla_info)\n",
    "for file in files[:10]:  # Primeros 10\n",
    "    print(f\"  {file.name}  ({file.size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Widgets - Par√°metros interactivos\n",
    "dbutils.widgets.text(\"nombre_usuario\", \"Estudiante\", \"Tu nombre\")\n",
    "dbutils.widgets.dropdown(\"nivel\", \"Intermedio\", [\"Principiante\", \"Intermedio\", \"Avanzado\"], \"Nivel\")\n",
    "\n",
    "# Obtener valores\n",
    "nombre = dbutils.widgets.get(\"nombre_usuario\")\n",
    "nivel = dbutils.widgets.get(\"nivel\")\n",
    "\n",
    "print(f\"üëã ¬°Hola {nombre}!\")\n",
    "print(f\"üìä Nivel seleccionado: {nivel}\")\n",
    "\n",
    "# Puedes ver los widgets en la parte superior del notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Visualizaciones en Databricks\n",
    "\n",
    "Databricks tiene visualizaciones integradas y soporta bibliotecas como Matplotlib, Seaborn, Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n nativa de Databricks\n",
    "# Simplemente usa display() con un DataFrame\n",
    "\n",
    "ventas_por_fecha = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        fecha,\n",
    "        SUM(total) as ingresos_diarios\n",
    "    FROM ventas_demo\n",
    "    GROUP BY fecha\n",
    "    ORDER BY fecha\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìà Ingresos diarios:\")\n",
    "display(ventas_por_fecha)\n",
    "\n",
    "# Despu√©s de ejecutar, haz clic en el √≠cono de gr√°fico debajo de la tabla\n",
    "# para crear visualizaciones interactivas (barras, l√≠neas, pie, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n con bibliotecas de Python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convertir a Pandas para visualizar\n",
    "ventas_pandas = ventas_por_producto.toPandas()\n",
    "\n",
    "# Crear gr√°fico\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=ventas_pandas, x='producto', y='ingresos_totales')\n",
    "plt.title('Ingresos Totales por Producto', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Producto')\n",
    "plt.ylabel('Ingresos ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Gr√°fico de barras creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Workflows y Jobs\n",
    "\n",
    "### ¬øQu√© son los Workflows?\n",
    "\n",
    "Secuencias de tareas automatizadas que se ejecutan:\n",
    "- En un horario (diario, semanal, mensual)\n",
    "- Por eventos (cuando llegan nuevos datos)\n",
    "- Manualmente\n",
    "\n",
    "### Ejemplo de Workflow:\n",
    "\n",
    "```\n",
    "1. [Extraer datos] ‚Üí 2. [Transformar] ‚Üí 3. [Cargar a tabla] ‚Üí 4. [Entrenar modelo]\n",
    "```\n",
    "\n",
    "### C√≥mo crear un Job:\n",
    "\n",
    "1. Ve a la secci√≥n **Workflows** en el men√∫ lateral\n",
    "2. Click en **Create Job**\n",
    "3. Configura:\n",
    "   - **Task**: Qu√© notebook ejecutar\n",
    "   - **Cluster**: Qu√© recursos usar\n",
    "   - **Schedule**: Cu√°ndo ejecutar\n",
    "   - **Alertas**: Notificaciones si falla\n",
    "\n",
    "### Ventajas:\n",
    "\n",
    "- ‚úÖ Automatizaci√≥n completa\n",
    "- ‚úÖ Retry autom√°tico si falla\n",
    "- ‚úÖ Notificaciones por email/Slack\n",
    "- ‚úÖ Dependencias entre tareas\n",
    "- ‚úÖ Historial de ejecuciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üó∫Ô∏è Navegaci√≥n del Workspace\n",
    "\n",
    "### Men√∫ Principal (Barra Lateral):\n",
    "\n",
    "| Secci√≥n | ¬øQu√© hace? |\n",
    "|---------|------------|\n",
    "| üè† **Workspace** | Tus notebooks y carpetas |\n",
    "| üî¨ **Data Science & Engineering** | √Årea de trabajo principal |\n",
    "| üíæ **Data** | Explorador de tablas y bases de datos |\n",
    "| üñ•Ô∏è **Compute** | Gestionar clusters |\n",
    "| üîÑ **Workflows** | Jobs automatizados |\n",
    "| ü§ñ **Machine Learning** | Experimentos y modelos MLflow |\n",
    "| üìä **SQL** | Editor SQL y dashboards |\n",
    "| ‚öôÔ∏è **Settings** | Configuraci√≥n del workspace |\n",
    "\n",
    "### Data Explorer:\n",
    "\n",
    "- Ver todas tus tablas y bases de datos\n",
    "- Explorar schemas y datos\n",
    "- Ver historial de versiones (Delta)\n",
    "- Gestionar permisos\n",
    "\n",
    "### Compute (Clusters):\n",
    "\n",
    "**Tipos de Clusters:**\n",
    "\n",
    "1. **All-Purpose**: Para desarrollo interactivo\n",
    "   - Se apaga autom√°ticamente\n",
    "   - M√∫ltiples usuarios pueden conectarse\n",
    "\n",
    "2. **Job Clusters**: Para producci√≥n\n",
    "   - Se crean para un job espec√≠fico\n",
    "   - Se destruyen al terminar\n",
    "   - M√°s econ√≥micos\n",
    "\n",
    "**Configuraci√≥n importante:**\n",
    "- **Autotermination**: Apagar despu√©s de X minutos inactivo\n",
    "- **Autoscaling**: Crecer/decrecer seg√∫n la carga\n",
    "- **Runtime**: Versi√≥n de Spark y librer√≠as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Crear tu primera tabla\n",
    "1. Crea un DataFrame con datos de estudiantes (nombre, edad, calificaci√≥n)\n",
    "2. Gu√°rdalo como tabla Delta\n",
    "3. Haz una consulta SQL para obtener el promedio de calificaciones\n",
    "\n",
    "### Ejercicio 2: Transformaciones Spark\n",
    "1. Lee la tabla de ventas que creamos\n",
    "2. Filtra solo las ventas de Laptops\n",
    "3. Calcula el ingreso promedio por transacci√≥n\n",
    "4. Ordena por fecha\n",
    "\n",
    "### Ejercicio 3: Time Travel\n",
    "1. Actualiza algunos datos en tu tabla\n",
    "2. Usa DESCRIBE HISTORY para ver las versiones\n",
    "3. Lee una versi√≥n anterior usando versionAsOf\n",
    "4. Compara los datos entre versiones\n",
    "\n",
    "### Ejercicio 4: Visualizaci√≥n\n",
    "1. Agrupa las ventas por producto\n",
    "2. Crea una visualizaci√≥n de barras con display()\n",
    "3. Crea un gr√°fico de l√≠neas con matplotlib mostrando ventas por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESPACIO PARA TUS EJERCICIOS\n",
    "\n",
    "# Ejercicio 1: Tu c√≥digo aqu√≠\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Recursos Adicionales\n",
    "\n",
    "### Documentaci√≥n Oficial:\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n",
    "- [Delta Lake Guide](https://docs.delta.io/)\n",
    "\n",
    "### Datasets de Pr√°ctica:\n",
    "Databricks incluye datasets de ejemplo en `/databricks-datasets`:\n",
    "- COVID-19 data\n",
    "- NYC Taxi trips\n",
    "- Amazon product reviews\n",
    "- Online retail transactions\n",
    "\n",
    "### Comandos √∫tiles:\n",
    "```python\n",
    "# Ver datasets disponibles\n",
    "display(dbutils.fs.ls(\"/databricks-datasets\"))\n",
    "\n",
    "# Cargar dataset de ejemplo\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/databricks-datasets/online_retail/data-001/data.csv\")\n",
    "```\n",
    "\n",
    "### Comunidad:\n",
    "- [Databricks Community Forum](https://community.databricks.com/)\n",
    "- [Stack Overflow - databricks tag](https://stackoverflow.com/questions/tagged/databricks)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Resumen de Conceptos Clave\n",
    "\n",
    "‚úÖ **Databricks** = Plataforma unificada para datos y ML  \n",
    "‚úÖ **Apache Spark** = Motor de procesamiento distribuido  \n",
    "‚úÖ **Delta Lake** = Almacenamiento con transacciones y versionado  \n",
    "‚úÖ **Notebooks** = Entorno interactivo de desarrollo  \n",
    "‚úÖ **Clusters** = Recursos de c√≥mputo escalables  \n",
    "‚úÖ **Workflows** = Automatizaci√≥n de pipelines  \n",
    "\n",
    "### Pr√≥ximos pasos:\n",
    "\n",
    "En el siguiente notebook aprender√°s sobre **MLflow**, la herramienta de Databricks para:\n",
    "- Rastrear experimentos de ML\n",
    "- Versionar modelos\n",
    "- Desplegar en producci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Felicidades! Has completado la introducci√≥n a Databricks** üéâ\n",
    "\n",
    "Contin√∫a con el siguiente notebook: **02_Introduccion_MLflow.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
