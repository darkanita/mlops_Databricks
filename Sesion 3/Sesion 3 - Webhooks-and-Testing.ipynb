{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dd5462-9182-4edc-acd4-a344331e5f82",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "\n",
    "# MLflow Webhooks & Testing\n",
    "\n",
    "Webhooks trigger the execution of code (oftentimes tests) upon some event. This lesson explores how to employ webhooks to trigger automated tests against models in the model registry. \n",
    "\n",
    "## In this lesson you will:<br>\n",
    " - Explore the role of webhooks in ML pipelines\n",
    " - Create a job to test models in the model registry\n",
    " - Automate that job using MLflow webhooks\n",
    " - Create a HTTP webhook to send notifications to Slack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98af2f25-c9b2-4aba-82f7-bf0d5df6de76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Automated Testing\n",
    "\n",
    "The backbone of the continuous integration, continuous deployment (CI/CD) process is the automated building, testing, and deployment of code. A **webhook or trigger** causes the execution of code based upon some event.  This is commonly when new code is pushed to a code repository.  In the case of machine learning jobs, this could be the arrival of a new model in the model registry.\n",
    "\n",
    "The two types of <a href=\"https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html\" target=\"_blank\">**MLflow Model Registry Webhooks**</a>:\n",
    " - Webhooks with Job triggers: Trigger a job in a Databricks workspace\n",
    " - Webhooks with HTTP endpoints: Send triggers to any HTTP endpoint\n",
    " \n",
    "This lesson uses:\n",
    "1. a **Job webhook** to trigger the execution of a Databricks job \n",
    "2. a **HTTP webhook** to send notifications to Slack \n",
    "\n",
    "Upon the arrival of a new model version with a given name in the model registry, the function of the Databricks job is to:<br><br>\n",
    "- Import the new model version\n",
    "- Test the schema of its inputs and outputs\n",
    "- Pass example code through the model\n",
    "\n",
    "This covers many of the desired tests for ML models.  However, throughput testing could also be performed using this paradigm. Also, the model could also be promoted to the production stage in an automated fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32558485-a6f4-416c-a7b0-5631cb76516b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create a Model and Job\n",
    "\n",
    "The following steps will create a Databricks job using another notebook in this directory: **`03b-Webhooks-Job-Demo`**\n",
    "\n",
    "**Note:** \n",
    "* Ensure that you are an admin on this workspace and that you're not using Community Edition (which has jobs disabled). \n",
    "* If you are not an admin, ask the instructor to share their token with you. \n",
    "* Alternatively, you can set **`token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735259e5-d279-4deb-98e7-90ad7eb3dce0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create a user access token\n",
    "\n",
    "Create a user access token using the following steps:<br><br>\n",
    "\n",
    "1. Click the Settings icon\n",
    "1. Click User Settings\n",
    "1. Go to the Access Tokens tab\n",
    "1. Click the Generate New Token button\n",
    "1. Optionally enter a description (comment) and expiration period\n",
    "1. Click the Generate button\n",
    "1. Copy the generated token **and paste it in the following cell**\n",
    "\n",
    "**Note:**\n",
    "* Ensure that you are an admin on this workspace and that you're not using Community Edition (which has jobs disabled). \n",
    "* If you are not an admin, ask the instructor to share their token with you. \n",
    "* Alternatively, you can set **`token = mlflow.utils.databricks_utils._get_command_context().apiToken().get()`**. However, this is not a best practice. We recommend you create your personal access token using the steps above and save it in your [secret scope](https://docs.databricks.com/security/secrets/secret-scopes.html). \n",
    "\n",
    "\n",
    "You can find details <a href=\"https://docs.databricks.com/dev-tools/api/latest/authentication.html\" target=\"_blank\">about access tokens here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c07df8f-43fe-4076-8381-d48b268ad2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    " \n",
    "token = \"<YOUR_DATABRICKS_TOKEN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585e9501-2d5a-448d-9549-a82036415af4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# With the token, we can create our authorization header for our subsequent REST calls\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "instance = mlflow.utils.databricks_utils.get_webapp_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee5eb0d-1cf0-49f0-9e55-8e7fa4120e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Train and Register a Model\n",
    "\n",
    "Build and log your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73db4791-8e50-45f2-bb50-78fefc597258",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/mlflow/models/signature.py:130: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  inputs = _infer_schema(model_input)\n/databricks/python/lib/python3.9/site-packages/mlflow/models/signature.py:131: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  outputs = _infer_schema(model_output) if model_output is not None else None\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with mlflow.start_run(run_name=\"Webhook RF Experiment\") as run:\n",
    "    # Data prep\n",
    "    white_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-white.csv\", sep=\";\")\n",
    "    red_wine = pd.read_csv(\"/dbfs/databricks-datasets/wine-quality/winequality-red.csv\", sep=\";\")\n",
    "\n",
    "    red_wine['is_red'] = 1\n",
    "    white_wine['is_red'] = 0\n",
    "\n",
    "    data = pd.concat([red_wine, white_wine], axis=0)\n",
    "\n",
    "    # Remove spaces from column names\n",
    "    data.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)\n",
    "\n",
    "    high_quality = (data.quality >= 7).astype(int)\n",
    "    data.quality = high_quality\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "\n",
    "    train, test = train_test_split(data, random_state=123)\n",
    "    X_train = train.drop([\"quality\"], axis=1)\n",
    "    X_test = test.drop([\"quality\"], axis=1)\n",
    "    y_train = train.quality\n",
    "    y_test = test.quality\n",
    "\n",
    "    signature = infer_signature(X_train, pd.DataFrame(y_train))\n",
    "    example = X_train.head(3)\n",
    "\n",
    "    # Train and log model\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(rf, \"random-forest-model\", signature=signature, input_example=example)\n",
    "    auc = roc_auc_score(y_test, rf.predict(X_test))\n",
    "    mlflow.log_metric(\"auc\", auc)\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0af7b66-242d-4f0a-bdbd-4b731174e64c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c24cdca3-ea56-4a95-b163-f643ebfd544d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'webhook-demo_aml' already exists. Creating a new version of this model...\n2024/04/11 10:42:49 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: webhook-demo_aml, version 4\nCreated version '4' of model 'webhook-demo_aml'.\n"
     ]
    }
   ],
   "source": [
    "suffix = \"aml\"\n",
    "name = f\"webhook-demo_{suffix}\"\n",
    "model_uri = f\"runs:/{run_id}/random-forest-model\"\n",
    "\n",
    "model_details = mlflow.register_model(model_uri=model_uri, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53d7225e-2e10-418c-b165-76999f116a63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Creating the Job\n",
    "\n",
    "The following steps will create a Databricks job using another notebook in this directory: **`Sesion 3 - Webhooks-Job-Demo`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06ae26b-d8f8-4922-bcb2-3d34b290c945",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Create a job that executes the notebook **`Sesion 3 - Webhooks-Job-Demo`** in the same folder as this notebook.<br><br>\n",
    "\n",
    "- Hover over the sidebar in the Databricks UI on the left. Click in **Job Runs**\n",
    "\n",
    "- Click on Create Job\n",
    "<br></br>\n",
    "  - Name your Job\n",
    "  - Select the notebook **`Sesion 3 - Webhooks-Job-Demo`** \n",
    "  - Select the current cluster\n",
    "<br></br>\n",
    "- Copy the Job ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c9590b6-905a-4470-8c01-ce2c721c90e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Alternatively, the code below will programmatically create the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb1a105-2629-4e78-b960-9b907423e999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def find_job_id(instance, headers, job_name, offset_limit=1000):\n",
    "    params = {\"offset\": 0}\n",
    "    uri = f\"{instance}/api/2.1/jobs/list\"\n",
    "    done = False\n",
    "    job_id = None\n",
    "    while not done:\n",
    "        done = True\n",
    "        res = requests.get(uri, params=params, headers=headers)\n",
    "        assert res.status_code == 200, f\"Job list not returned; {res.content}\"\n",
    "        \n",
    "        jobs = res.json().get(\"jobs\", [])\n",
    "        if len(jobs) > 0:\n",
    "            for job in jobs:\n",
    "                if job.get(\"settings\", {}).get(\"name\", None) == job_name:\n",
    "                    job_id = job.get(\"job_id\", None)\n",
    "                    break\n",
    "\n",
    "            # if job_id not found; update the offset and try again\n",
    "            if job_id is None:\n",
    "                params[\"offset\"] += len(jobs)\n",
    "                if params[\"offset\"] < offset_limit:\n",
    "                    done = False\n",
    "    \n",
    "    return job_id\n",
    "\n",
    "def get_job_parameters(job_name, cluster_id, notebook_path):\n",
    "    params = {\n",
    "            \"name\": job_name,\n",
    "            \"tasks\": [{\"task_key\": \"webhook_task\", \n",
    "                       \"existing_cluster_id\": cluster_id,\n",
    "                       \"notebook_task\": {\n",
    "                           \"notebook_path\": notebook_path\n",
    "                       }\n",
    "                      }]\n",
    "        }\n",
    "    return params\n",
    "\n",
    "def get_create_parameters(job_name, cluster_id, notebook_path):\n",
    "    api = \"api/2.1/jobs/create\"\n",
    "    return api, get_job_parameters(job_name, cluster_id, notebook_path)\n",
    "\n",
    "def get_reset_parameters(job_name, cluster_id, notebook_path, job_id):\n",
    "    api = \"api/2.1/jobs/reset\"\n",
    "    params = {\"job_id\": job_id, \"new_settings\": get_job_parameters(job_name, cluster_id, notebook_path)}\n",
    "    return api, params\n",
    "\n",
    "def get_webhook_job(instance, headers, job_name, cluster_id, notebook_path):\n",
    "    job_id = find_job_id(instance, headers, job_name)\n",
    "    if job_id is None:\n",
    "        api, params = get_create_parameters(job_name, cluster_id, notebook_path)\n",
    "    else:\n",
    "        api, params = get_reset_parameters(job_name, cluster_id, notebook_path, job_id)\n",
    "    \n",
    "    uri = f\"{instance}/{api}\"\n",
    "    res = requests.post(uri, headers=headers, json=params)\n",
    "    assert res.status_code == 200, f\"Expected an HTTP 200 response, received {res.status_code}; {res.content}\"\n",
    "    job_id = res.json().get(\"job_id\", job_id)\n",
    "    return job_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60aebda-b0b4-45a5-99fe-0f34db7c1eed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID:   338420584022521\nJob name: session3_webhook-job\n"
     ]
    }
   ],
   "source": [
    "notebook_path = mlflow.utils.databricks_utils.get_notebook_path().replace(\"Sesion 3 - Webhooks-and-Testing\", \"Sesion 3 - Webhooks-Job-Demo\")\n",
    "\n",
    "# We can use our utility method for creating a unique \n",
    "# database name to help us construct a unique job name.\n",
    "prefix = \"session3\"\n",
    "job_name = f\"{prefix}_webhook-job\"\n",
    "\n",
    "# if the Job was created via UI, set it here.\n",
    "job_id = get_webhook_job(instance, \n",
    "                         headers, \n",
    "                         job_name,\n",
    "                         spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\"),\n",
    "                         notebook_path)\n",
    "\n",
    "print(f\"Job ID:   {job_id}\")\n",
    "print(f\"Job name: {job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43d44ac-347a-4032-bb6f-dcbbe80f92b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create a Job Webhook\n",
    "\n",
    "There are a few different events that can trigger a Webhook. In this notebook, we will be experimenting with triggering a job when our model transitions between stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed84afe1-dc8a-4485-9247-c2675e7eda23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_id = 47669448112835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2d0e98-d092-4a22-b257-33243f89935f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from mlflow.utils.rest_utils import http_request\n",
    "from mlflow.utils.databricks_utils import get_databricks_host_creds\n",
    "\n",
    "endpoint = \"/api/2.0/mlflow/registry-webhooks/create\"\n",
    "host_creds = get_databricks_host_creds(\"databricks\")\n",
    "\n",
    "job_json = {\"model_name\": name,\n",
    "            \"events\": [\"MODEL_VERSION_TRANSITIONED_STAGE\"],\n",
    "            \"description\": \"Job webhook trigger\",\n",
    "            \"status\": \"Active\",\n",
    "            \"job_spec\": {\"job_id\": job_id,\n",
    "                         \"workspace_url\": instance,\n",
    "                         \"access_token\": token}\n",
    "           }\n",
    "\n",
    "response = http_request(\n",
    "    host_creds=host_creds, \n",
    "    endpoint=endpoint,\n",
    "    method=\"POST\",\n",
    "    json=job_json\n",
    ")\n",
    "assert response.status_code == 200, f\"Expected HTTP 200, received {response.status_code}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "660acb6e-1c06-4d58-8832-a5f32788a50e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Now that we have registered the webhook, we can **test it by transitioning our model from stage `None` to `Staging` in the Experiment UI.** We should see in the Jobs tab that our Job has run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b8a3397-d19c-4d8e-98ab-1221cb07d92b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "To get a list of active Webhooks, use a GET request with the LIST endpoint. Note that this command will return an error if no Webhooks have been created for the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c68d29c1-cf58-4a03-ac24-f30b8f3b2f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n    \"webhooks\": [\n        {\n            \"id\": \"2ea9474bee6147888985c2033f9cb55e\",\n            \"events\": [\n                \"MODEL_VERSION_TRANSITIONED_STAGE\"\n            ],\n            \"creation_timestamp\": 1712832263711,\n            \"last_updated_timestamp\": 1712832263711,\n            \"description\": \"Job webhook trigger\",\n            \"status\": \"ACTIVE\",\n            \"job_spec\": {\n                \"job_id\": \"47669448112835\",\n                \"workspace_url\": \"https://eastus-c3.azuredatabricks.net\"\n            },\n            \"model_name\": \"webhook-demo_aml\"\n        },\n        {\n            \"id\": \"0fb3711043f149968fba4dc72b7b4296\",\n            \"events\": [\n                \"MODEL_VERSION_TRANSITIONED_STAGE\"\n            ],\n            \"creation_timestamp\": 1712612412391,\n            \"last_updated_timestamp\": 1712612412391,\n            \"description\": \"Job webhook trigger\",\n            \"status\": \"ACTIVE\",\n            \"job_spec\": {\n                \"job_id\": \"338420584022521\",\n                \"workspace_url\": \"https://eastus-c3.azuredatabricks.net\"\n            },\n            \"model_name\": \"webhook-demo_aml\"\n        }\n    ]\n}\n"
     ]
    }
   ],
   "source": [
    "endpoint = f\"/api/2.0/mlflow/registry-webhooks/list/?model_name={name.replace(' ', '%20')}\"\n",
    "\n",
    "response = http_request(\n",
    "    host_creds=host_creds, \n",
    "    endpoint=endpoint,\n",
    "    method=\"GET\"\n",
    ")\n",
    "assert response.status_code == 200, f\"Expected HTTP 200, received {response.status_code}\"\n",
    "\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a506b4-d7f9-4a7b-8390-87ba433e152e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Finally, delete the webhook by copying the webhook ID to the curl or python request. You can confirm that the Webhook was deleted by using the list request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f008b0-cabd-4257-bd15-486ece6e1fd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "delete_hook = \"<insert your webhook id here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7752ce57-57fe-4926-a16b-8857dfde0503",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_json = {\"id\": delete_hook}\n",
    "endpoint = f\"/api/2.0/mlflow/registry-webhooks/delete\"\n",
    "\n",
    "response = http_request(\n",
    "    host_creds=host_creds, \n",
    "    endpoint=endpoint,\n",
    "    method=\"DELETE\",\n",
    "    json=new_json\n",
    ")\n",
    "assert response.status_code == 200, f\"Expected HTTP 200, received {response.status_code}\"\n",
    "\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sesion 3 - Webhooks-and-Testing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
