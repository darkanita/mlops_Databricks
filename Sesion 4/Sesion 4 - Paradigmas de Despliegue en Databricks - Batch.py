# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC # Batch Deployment
# MAGIC
# MAGIC Batch inference is the most common way of deploying machine learning models.  This lesson introduces various strategies for deploying models using batch including Spark, write optimizations, and on the JVM.
# MAGIC
# MAGIC ## What we will learn:<br>
# MAGIC  - Explore batch deployment options
# MAGIC  - Apply an **`sklearn`** model to a Spark DataFrame and save the results
# MAGIC  - Employ write optimizations including partitioning and Z-ordering

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC
# MAGIC ### Inference in Batch
# MAGIC
# MAGIC Batch deployment represents the vast majority of use cases for deploying machine learning models.<br><br>
# MAGIC
# MAGIC * This normally means running the predictions from a model and saving them somewhere for later use.
# MAGIC * For live serving, results are often saved to a database that will serve the saved prediction quickly. 
# MAGIC * In other cases, such as populating emails, they can be stored in less performant data stores such as a blob store.
# MAGIC
# MAGIC <img src="https://files.training.databricks.com/images/eLearning/ML-Part-4/batch-predictions.png" width=800px />
# MAGIC
# MAGIC Writing the results of an inference can be optimized in a number of ways...<br><br>
# MAGIC
# MAGIC * For large amounts of data, predictions and writes should be performed in parallel
# MAGIC * **The access pattern for the saved predictions should also be kept in mind in how the data is written**
# MAGIC   - For static files or data warehouses, partitioning speeds up data reads
# MAGIC   - For databases, indexing the database on the relevant query generally improves performance
# MAGIC   - In either case, the index is working similar to an index in a book: it allows you to skip ahead to the relevant content

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC There are a few other considerations to ensure the accuracy of a model...<br><br>
# MAGIC
# MAGIC * First is to make sure that the model matches expectations
# MAGIC   - We'll cover this in further detail in the model drift section
# MAGIC * Second is to **retrain the model on the majority of your dataset**
# MAGIC   - Either use the entire dataset for training or around 95% of it
# MAGIC   - A train/test split is a good method in tuning hyperparameters and estimating how the model will perform on unseen data
# MAGIC   - Retraining the model on the majority of the dataset ensures that you have as much information as possible factored into the model

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### Inference in Spark
# MAGIC
# MAGIC Models trained in various machine learning libraries can be applied at scale using Spark.  To do this, use **`mlflow.pyfunc.spark_udf`** and pass in the **`SparkSession`**, name of the model, and run id.
# MAGIC
# MAGIC Using UDF's in Spark means that supporting libraries must be installed on every node in the cluster.  In the case of **`sklearn`**, this is installed in Databricks clusters by default.  When using other libraries, you will need to install them to ensure that they will work as UDFs.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC Start by training an **`sklearn`** model.  Apply it using a Spark UDF generated by **`mlflow`**.
# MAGIC
# MAGIC Import the data.  **Do not perform a train/test split.**
# MAGIC
# MAGIC It is common to skip the train/test split in training a final model.

# COMMAND ----------

from mlflow.models.signature import infer_signature
from sklearn.metrics import roc_auc_score
import mlflow.sklearn
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Data prep
white_wine = pd.read_csv("/dbfs/databricks-datasets/wine-quality/winequality-white.csv", sep=";")
red_wine = pd.read_csv("/dbfs/databricks-datasets/wine-quality/winequality-red.csv", sep=";")

red_wine['is_red'] = 1
white_wine['is_red'] = 0

data = pd.concat([red_wine, white_wine], axis=0)

# Remove spaces from column names
data.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)

high_quality = (data.quality >= 7).astype(int)
data.quality = high_quality

data.dropna(inplace=True)
data.reset_index(drop=True,inplace=True)

X = data.drop(["quality"], axis=1)
y = data.quality

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC Train and log model

# COMMAND ----------

with mlflow.start_run(run_name="Final RF Model") as run:
    rf = RandomForestClassifier(n_estimators=100, max_depth=5)
    rf.fit(X, y)

    predictions = rf.predict(X)
    mlflow.sklearn.log_model(rf, "random_forest_model")

    auc = roc_auc_score(y, predictions) # This is on the same data the model was trained
    mlflow.log_metric("auc", auc)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC
# MAGIC Create a Spark DataFrame from the Pandas DataFrame.

# COMMAND ----------

spark_df = spark.createDataFrame(X)
display(spark_df)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC MLflow easily produces a Spark user defined function (UDF).  This bridges the gap between Python environments and applying models at scale using Spark.

# COMMAND ----------

predict = mlflow.pyfunc.spark_udf(spark, f"runs:/{run.info.run_id}/random_forest_model")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC Apply the model as a standard UDF using the column names as the input to the function.

# COMMAND ----------

prediction_df = spark_df.withColumn("prediction", predict(*spark_df.columns))

display(prediction_df)

# COMMAND ----------

# MAGIC %md 
# MAGIC
# MAGIC ### Write Optimizations
# MAGIC
# MAGIC There are many possible optimizations depending on your batch deployment scenerio.  In Spark and Delta Lake, the following optimizations are possible:<br><br>
# MAGIC
# MAGIC - **Partitioning:** stores data associated with different categorical values in different directories
# MAGIC - **Z-Ordering:** colocates related information in the same set of files
# MAGIC - **Data Skipping:** aims at speeding up queries that contain filters (WHERE clauses)
# MAGIC - **Partition Pruning:** speeds up queries by limiting the amount of data read
# MAGIC
# MAGIC Other optimizations include:<br><br>
# MAGIC
# MAGIC - **Database indexing:** allows certain table columns to be more effectively queried 
# MAGIC - **Geo-replication:** replicates data in different geographical regions

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC Partition by is_red.

# COMMAND ----------

dbutils.fs.rm(f"/batch-predictions-partitioned.delta", recurse=True)
delta_partitioned_path = f"/batch-predictions-partitioned.delta"

prediction_df.write.partitionBy("is_red").mode("OVERWRITE").format("delta").save(delta_partitioned_path)

# COMMAND ----------

# MAGIC %md 
# MAGIC
# MAGIC Take a look at the files.

# COMMAND ----------

display(dbutils.fs.ls(delta_partitioned_path))

# COMMAND ----------

# MAGIC %md 
# MAGIC Z-Ordering is a form of multi-dimensional clustering that colocates related information in the same set of files.  It reduces the amount of data that needs to be read.  <a href="https://docs.databricks.com/delta/optimizations/file-mgmt.html#z-ordering-multi-dimensional-clustering" target="_blank">You can read more about it here.</a> Let's z-order by zipcode.

# COMMAND ----------

spark.sql(f"OPTIMIZE delta.`{delta_partitioned_path}` ZORDER BY (alcohol)")

# COMMAND ----------

# MAGIC %md 
# MAGIC
# MAGIC ### Feature Store Batch Scoring

# COMMAND ----------

!ls -la

# COMMAND ----------

# MAGIC %md 
# MAGIC Create feature table
# MAGIC
# MAGIC Read dataframe from the csv file with spark directly to track **`data source`** in Feature Store

# COMMAND ----------

from databricks import feature_store
from databricks.feature_store import feature_table,FeatureLookup

## create a feature store client
fs = feature_store.FeatureStoreClient()

# COMMAND ----------

from pyspark.sql.functions import monotonically_increasing_id
from pyspark.sql import functions as F

## build feature dataframe, add index column and drop label
white_wine = (spark.read.option("delimiter", ";")
           .csv("dbfs:/databricks-datasets/wine-quality/winequality-white.csv", header=True, inferSchema=True)
           .withColumn("index", monotonically_increasing_id()))

# Remove spaces from column names
for col in white_wine.columns:
    white_wine = white_wine.withColumnRenamed(col, col.replace(' ', '_'))

white_wine = white_wine.withColumn("quality", (F.col("quality") >= 7).cast("integer"))

white_wine = white_wine.withColumn('is_red', F.lit(0))

red_wine = (spark.read.option("delimiter", ";")
           .csv("dbfs:/databricks-datasets/wine-quality/winequality-white.csv", header=True, inferSchema=True)
           .withColumn("index", monotonically_increasing_id()))

# Remove spaces from column names
for col in red_wine.columns:
    red_wine = red_wine.withColumnRenamed(col, col.replace(' ', '_'))

red_wine = red_wine.withColumn("quality", (F.col("quality") >= 7).cast("integer"))

red_wine = red_wine.withColumn('is_red', F.lit(1))

df = red_wine.union(white_wine)

df = df.withColumn("index", monotonically_increasing_id())

## feature data - all the columns except for the true label
features_df = df.drop("quality")

## inference data - contains only index and label columns, if you have online features, it should be added to inference_df as well
inference_df = df.select("index", "quality")

# COMMAND ----------

display(df)

# COMMAND ----------

# MAGIC %md 
# MAGIC Declare a fully-qualified, unique table name.
# MAGIC
# MAGIC In DBR 10.5+, we can drop Feature Store tables, but for now we need a uniuqe name in case we re-run this notebook.

# COMMAND ----------

feature_table_name = f"default.wine_fs"
print(f"Table: {feature_table_name}\n")

# create feature table
result = fs.create_table(
    name=feature_table_name,
    primary_keys=["index"],
    df=features_df,
    description="review cols of wine data"
)

# COMMAND ----------

# MAGIC %md <i18n value="6ad326c5-036f-4c5e-950a-5ade154cc398"/>
# MAGIC
# MAGIC
# MAGIC
# MAGIC Create training set from feature store using **`fs.create_training_set`**

# COMMAND ----------

feature_lookups = [FeatureLookup(table_name = feature_table_name, feature_names = None, lookup_key = "index") ]

## fs.create_training_set will look up features in model_feature_lookups with matched key from inference_data_df
training_set = fs.create_training_set(inference_df, feature_lookups, label="quality", exclude_columns="index")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC Log a feature store packaged model.
# MAGIC
# MAGIC We need a unique model name and we can use our unique database name to construct it.

# COMMAND ----------

suffix = "amls4"
model_name = f"wine-fs-model_{suffix}"

print(f"Model Name: {model_name}")

# COMMAND ----------

from mlflow.models.signature import infer_signature
## log RF model as a feature store packaged model and register the packaged model in model registry as `model_name`
fs.log_model(
    model=rf,
    artifact_path="feature_store_model",
    flavor=mlflow.sklearn,
    training_set=training_set,
    registered_model_name=model_name,
    input_example=X[:5],
    signature=infer_signature(X, y)
)

# COMMAND ----------

# MAGIC %md 
# MAGIC
# MAGIC Let's now perform batch scoring with the feature store model.

# COMMAND ----------

## for simplicity sake, we will just predict on the same inference_data_df
batch_input_df = inference_df.drop("quality") #exclude true label
with_predictions = fs.score_batch(f"models:/{model_name}/2", 
                                  batch_input_df, result_type='double')
display(with_predictions)

# COMMAND ----------

# MAGIC %md
# MAGIC ## It is time to practice

# COMMAND ----------

from sklearn import datasets

housing = datasets.fetch_california_housing()
wbcd = wisconsin_breast_cancer_data = datasets.load_breast_cancer()